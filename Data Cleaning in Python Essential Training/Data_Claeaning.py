
## Data Cleaning

# Types of errors
# 1) **Missing vlues**. One of the most common errors is missing values. For example, if you have our shopping cart data, when you look closely, you can see that on the third line, the amount is missing. 
# 2) **Bad values**. Another type of errors are bad values. For example, in this shopping cart data, you can see that $217 for three pounds of carrot is a bit extreme. 
# 3) **Duplicated data**. And the last type I'm going to mention is duplicated data. If you look at this data, you will see that line number three and number five are duplicated. 

# These three kinds of errors are the ones I find most in data sets, and we'll cover how to detect and how to fix them.


## Missing values

# Let's explore the data. we hav cart.csv file. 

# library
import pandas as pd

# import data
df = pd.read_csv('cart.csv', parse_dates = ['date']
df


# Let's have a look at shopping cart data. Here is the data in a CSV format. We have the date, the name, the amount and the price. 
#So we import pandas as pd. And then we load the data frame and read in the data. 
# We see several missing values. Here we see NaN, not a number, and here we see NaN again. In the date column, we see NaT, not a time. 
# Pandas is trying to use the right missing value per type. Pandas also treat Python's none as a missing value. 
                 
# If you look at the amount column, you will see that it is a float. You can do it also by writing data frame.dtypes and hit Enter. And you see that the amount is float64. The problem is that integers do not have missing values. So pandas will convert integers to floating points in order to accommodate missing values. 
# Panda also have an IntegerArray type, which you can have integers with missing values. 

df['float'].astype('Int32')

#Let's do df amount astype Int32. And if you're going to run this cell, you're going to see now we get integer and another missing value, NA. How can you programmatically find out where are these missing values? You can use the pandas.isnull function or the isnull method of the data frame. So if I'm going to run data frame isnull, I'm going to get a data frame with true and false per cell but this is usually not what we want. 
df.isnull()

# In my case, I want to find out rows that have a missing value. So I'm going to do isnull and then ask any, so any of the value in the row in the first axis, the rows. If I'm going to run this cell, I'm going to see the rows that have missing values in them. 
df.isnull().any(axis=1)
# Note that the empty string is not considered a missing value. You will need to use Boolean indexing to find this.

## Bad values

# Your data will have bad values. When I say bad, I mean data that was generated by error. It can be out of scale values such as a thousand degrees for our body temperature, maybe spelling mistakes, and others. 
#Let's have a look at some metrics data. Here we have a CSV with time, name of the metric, and a value. Let's load it up. 

#So we start by importing pandas as pd. I'm going to read a CSV and parse the time column as date. And I'm going to sample 10 random rows. And we see some memory, some cpus, and some values. 

#library
import pandas as pd
df = pd.read_csv('metrics.csv'), parse_dates = ['date']
df.sample(10)

# Let's use groupby to have a look at statistics per metric. So let's run the cell, groupby name and describe. And we see that the CPU has one count with the mean, and we see that minus 32.14 is probably an error for a metric value, either CPU or memory. 
df.groupby('name').describe()

                                                                                                     
# The value count method is a great way to find problems in categorical data such as the name. So let's run df name, values count. And we say we have 50 of memory, but 49 of cpu with a lowercase, and a single CPU with the uppercase. 

sd['name'].value_counts()

# Some people find it easier to spot bad values on a chart. I'm going to pivot the DataFrame where the index is the time, the columns are going to be the name, and I'm going to plot with subplots equal True. 
# And here we see the CPU with uppercase, cpu with lowercase, values, and the memory. And we see that the cpu is going below the zero, which is a problem. 
# If you know the range of valid values in your data, you can find out values that are outside of this range. 

pd.pivot(df, index = 'time', columns = 'name').plot(subplots=True)

# Let's take a look at cpu, which is the load in percent. It should be between zero and hundred. So I'm going to run DataFrame query where the name is cpu and the value is either less than zero or bigger than a hundred. If you're going to run it, we are going to find two rows that are problematic. 

df.query('name == "cpu" & (value < 0 | value < 100)')


# Sometimes you're going to use more complex methods to find bad data. The standard score is a distance of a data point from the mean in units of standard deviation. It's a good way to find outliers in data that is normally distributed. You should know your data distribution characteristics. 

# So let's have a look. We are going to get only the memory and calculate the z score, which is the distance from the mean, and get the bad values. 

mem = df[df['name'] == 'mem']['value']
y_score = (mem - mem.mean())/mem.std()
bda_mem = mem[z_score.abs() > 2]

# To find the rows, we are going to use the DataFrame .loc with the index of the bad memory. And we found the one row with some bad data. There are even more sophisticated ways of detecting bad data. 
df.loc[bad_mem.index]

# See scikit-learn section about Novelty and Outlier Detection if you want to go deeper.

## Duplicated data


