{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision tree is supervised machine learning approach. This means that we can use a decision tree to predict future outcomes. Because of their versatility and easy of use, decision trees are one of the most widely used machine learning approaches.  \n",
    "Decision tree is collection of nodes which are connected to other decision nodes or leaf nodes by branches. \n",
    "\n",
    "We can interpret the structure of a decision tree as a set of rules or guiding principles. \n",
    "\n",
    "Decision trees are well suited for applications that require transparency. \n",
    "\n",
    "Decision Trees: \n",
    "+ Cllassification Trees. where the dependent variable is categorical or discreet value such as color, true or false, yes or no, then the type of tree we build is called a classification tree. \n",
    "\n",
    "+ Regression Trees. if our dependent variable is a continuos value, such as age, income, temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees\n",
    "\n",
    "Classification trees are built using a process known as recursive partitioning. The basic idea behind this process is to repeatedly split data into smaller subsets in such a way that maximizes the homogeneity or similarity of items within each subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Partitioning\n",
    "\n",
    "Data is recursively split into smaller subsets until:\n",
    "+ All data in a partition are of the  same class\n",
    "+ All features have been exhausted\n",
    "+ A specified tree size limit or other user-defined condition has been met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Impurity\n",
    "\n",
    "+ Classification tree algorithms use a mathematical formula as a measure of the degree of impurity within a partition. \n",
    "+ two of the most commonly used measures of impurity are entropy and Gini. \n",
    "\n",
    "## Entrophy as a measure of Impurity\n",
    "\n",
    "+ Entropy (used by C5.0) is a qualification of the level of randomness or disorder within a partition. \n",
    "+ Low entropy implies large homogeneity (similarity) in outcomes within a partition. \n",
    "+ High entropy implies large heterogeneity (dissimilarity) in outcomes within a partition\n",
    "\n",
    "\n",
    "## Information Gain\n",
    "\n",
    "It is the difference between the entropy of the partition before the split and the combined entropy of the partitions after the split. The algorithm will choose the split that results in the largest information gain or reduction in entropy. That is the best split.\n",
    "\n",
    "## Gini as a Measure of Impurity\n",
    "\n",
    "+ The Gini impurity measure (used by CRT) is a measure f statistical dispersion.\n",
    "+ The greater the degree of randomness within a partition, the higher the Gini impurity value. \n",
    "+ The samller the degree of randomness within a partition, the lower the gini impurity value. \n",
    "\n",
    "Gini is the preferred measure of impurity for the CART decision tree algorithm. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree\n",
    "\n",
    "The first thing a regression tree algorithm does is figure out how best to split this data into two so we have partitions or subsets that minimize variablity the most. \n",
    "\n",
    "One of the measures that regression tree alorithms rely on to figure out thebest split is the sum of squared residulas or SSR.\n",
    "\n",
    "\n",
    "## Sum of Squared Residuals (SSR)\n",
    "+ SSR is a quantification of the overall difference between the values in a partition and the average value of the partition. \n",
    "+ High SSR implies that the values in the partition are dissimilar or very different from the mean. \n",
    "+ Low SSR implies that the values in the partition are similar, or close to the mean. \n",
    "\n",
    "*** The regression tree algorithm evaluates the SSR based on each possible split, and chooses the one with the lowest SSR. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prune a decision tree?\n",
    "\n",
    "*** Overfitting occurs when a decision tree fits our data too perfectly. \n",
    "\n",
    "# Pre-pruning\n",
    "We can limit the size of a tree during the recursive partitioning process. \n",
    "\n",
    "*** Pre-pruning is efficient but my result in important patterns being missed.\n",
    "\n",
    "# Post-pruning\n",
    "\n",
    "We can limit the size of a tree after the recursive partitioning process.\n",
    "\n",
    "# Tree score or Cost complexity measure\n",
    "\n",
    "Cost complexity measure for a tree is the sum of squared residualy for the tree plus the tree comlexity penalty. \n",
    "\n",
    "The tree complexity penalty compensates for the number of leaf nodes in the tree. \n",
    "\n",
    "The choice of alpha has a significant impact on which tree ends up having the lowest tree score. Higher values for alpha favor simpler trees, while smaller values for alpha favor more complex trees. \n",
    "\n",
    "How do we find the best value for alpha? \n",
    "\n",
    "The simple answer is that alpha is a hyperparameter and we use a process known as hyperparameter tuning to find the best value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why and when to use a decision tree\n",
    "\n",
    "Strengs: \n",
    "+ The decision tree structure is easy to understand. \n",
    "+ Decision trees are useful for both classification and regression problems. \n",
    "+ Decision trees require very little data preprocessing, no need to handle missing, noisy and outlier data. \n",
    "+ Feature selection is not necessary with decision trees. \n",
    "+ Decision tree do well on most problems, even if it makes slightly erroneous assumptions, about the nature of the data that is used to build it.\n",
    "+ Decision trees work well with small and large data sets.\n",
    "\n",
    "***However, similar to other non-parametric models, the predictive accuracy of a decision tree, tends to improve as it encounters more training examples. \n",
    "\n",
    "Weaknesses: \n",
    "+ Splits are sometimes toward features with a large number of unique values - when using entropy as a measure of impurity.\n",
    "+ Samll changes in the data can result in large changes to the tructure of a decision tree.\n",
    "+ It is easy to overfit or underfit data with a decision tree.\n",
    "+ Decision trees are limited to axis-parallel splits. This means that you can only split data,  horizontaly or vertically, during the recursive partitioning process.  \n",
    "+ Large decision trees can be difficult to interpret or understand. \n",
    "+ Class imbalance leads to biased decision trees. If the training data suffers from class imbalance. To mitigate this, we often have to balance the training data, before trying to fit a decision tree to it."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
