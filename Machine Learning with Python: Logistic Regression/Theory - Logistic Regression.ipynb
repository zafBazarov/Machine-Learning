{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is logistic regression?\n",
    "\n",
    "Logistic regression is a supervised machine learning technique that models the relationship between one or more predictors, and the probability of a categorical response. To help motivate our understanding of how logistic regression works, let's suppose that we intend to build a regression model that predicts whether a particular customer will or will not rent an e-bike, as a function of their age. \n",
    "\n",
    "A sample of the historical data that we use to such a model could look something like this. If we plot this data, we get a scatter plot that looks like this. Note, that the response values yes and no, a categorical values, they are not continuous values as we normally expect for regression problems. To deal with this, instead of modeling the response values directly, logistic regression models the probability of a particular response value. \n",
    "\n",
    "In this example, a logistic regression algorithm would model the probability that the value of the response variable rent is yes given an age value, because the probability of an event goes from zero to one, if we represent yes as one and no as zero, the objective now becomes predicting the probability of one given an age value. \n",
    "\n",
    "Notice, by approaching the problem this way, the response is no longer categorical, it has become continuous. The logistic regression algorithm uses a nonlinear function, known as a logistic function to fit a curve to the data. The curve is bounded on both ends by zero and one, and is known as a sigmoid curve. \n",
    "\n",
    "Similar to other regression algorithms, the logistic regression algorithm uses a series of mathematical transformations to find the better coefficients, that result in a curve that best fits the data. \n",
    "\n",
    "The process it uses is known as maximum likelihood estimation or MLE. The mechanics of how MLE works is beyond the scope of this course. Using the curve of best fit, we can visually estimate the probability of an event for any given X, by projecting from the x-axis to the y-axis. For example, according to the fitted curve, the probability of a 35 year old renting an e-bike is essentially 0%, while the probability of a 45 year old renting an e-bike is 100%.\n",
    "\n",
    "To convert these predicted probability values to a categorical response, logistic regression algorithms use what is known as a cutoff threshold. In this example, response values at or above the threshold are interpreted as yes, while response values below the threshold are interpreted as no. So we can interpret the prediction for a 35 year old customer as no, and that of a 45 year old customer as yes. \n",
    "\n",
    "It's important to note that while most logistic regression models use 0.5 as a default cutoff, we can adjust this threshold depending on our data, and how conservative we want to be with our interpretation. In the example shown here, with a threshold set at 0.9, our response of 0.8 would still be interpreted as no. \n",
    "\n",
    "## Logistic Regression Subcategories\n",
    "\n",
    "Logistic regression is a very popular technique for solving classification problems. Depending on the characteristics of the response variable, the logistic regression model we build will likely fall into one of three subcategoriess: \n",
    "+ The most common type of logistic regression is binomial logistic regression. This type of model is useful in predicting outcomes that only have two possible values, such as yes or no, true or false, one or zero. \n",
    "Most problems fall into this category. Our e-bike rental example is one of them. \n",
    "+ The second type of logistic regression is known as multinomial logistic regression. These types of models are useful when our response has more than two possible values. For example, a multinomial logistic regression model would be useful in predicting which one out of three medical conditions a patient is at the highest risk for. With multinomial logistic regression, we typically break down the problem by modeling the probability of each response value independently. Then we choose the one with the highest probability. \n",
    "+ The third type of logistic regression is ordered logistic regression. This approach is very similar to multinomial logistic regression with one key difference, the response values have a logical order. An example of an ordered response is a Likert scale of one to five. An ordered logistic regression model would be useful in predicting the rating or the number of stars, a student would give to a LinkedIn learning course based on the number of coding examples in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Logistic regression is most often used to solve classification problems because that allows us to predict the probability of a categorical value Y for any given X. \n",
    "+ For example. Suppose that we have a logistic regression function like this that models the probability of a customer renting an e-bike as a function of their age. By doing the math, we can predict that the probability of a 35 year old customer renting an e-bike is 14.8%. If we use a 50% cutoff threshold then we interpret the prediction of 0.148 as no. According to this model, a 35 year old customer is unlikely to rent an e-bike. \n",
    "\n",
    "Statistically speaking, modeling the value of a variable with a restricted range, such probability can be challenging. \n",
    "\n",
    "*** It is also rather difficult to properly interpret the non-linear relationship that exists between predictors and the response as represented by the logistic function. \n",
    "\n",
    "So instead of using the logistic function, most logistic regression algorithms actually model the Log Odds of the response as a linear combination of the predictors as shown here. In this equation, P is a logistic function and the Log Odds of the logistic function is known as a Logit, unlike probability, which ranges from zero to one, Log Odds values range from negative infinity to positive infinity, they are not bounded on either side. The idea of odds, Log Odds and probability can sometimes be confusing. \n",
    "\n",
    "To help clear up some of the confusion, let's explore the relationship between all three measures. \n",
    "+ Odds ratios are commonly used in domains such as horse racing, sports, epidemiology, and gambling. In sports, instead of stating the probability of winning, people will often talk about the odds of winning. \n",
    "\n",
    "ODDS\n",
    "The odds or odds ratio of an event is a probability that the event will occur expressed as a proportion of the probability that the event will not occur. \n",
    "\n",
    "So if the probability of an event is P, then the odds of the event is P divided by one minus P. To illustrate, let's suppose that out of 10 basketball games, team A won eight and team B won two. Based on these results, we can say that the probability of team A winning a game against team B is eight divided by 10 or 80%. Conversely, the probability of team A losing a game to team B is 20%. \n",
    "\n",
    "Note that this is also the probability of team B winning a game against team A. In terms of odds, we say that the odds of team A winning a game against team B is four to one, which is 0.8 divided by 0.2. In other words, team A's odds of winning is four times that of Team B. Taking the log of the odds of an event gives us the Logit function. To go from Log Odds to odds, we exponentiate. And to go from odds to probability, we divide the odds by one plus the odds. Notice that this is the Logistic Function. \n",
    "\n",
    "As I mentioned earlier, instead of modeling the probability of a response, most logistic regression algorithms model the Log Odds of the response as a linear combination of the predictors. \n",
    "\n",
    "The scikit-learn Python package which we will use later in this course, uses this approach. So if we train a model in Python that estimates the probability of a customer renting an e-bike as a function of their age, the function would look like this. To predict the probability of a 35 year old customer renting an e-bike, the model would first calculate the Log Odds of the event which is negative 1.75. To get the odds, it exponentiates the Log Odds, which turns out to be 0.1738. Finally, it divides the odds by one plus the odds to get a probability of 0.148. As you can see, we end up with the same probability: 14.8% as we did earlier in the video when we used the logistic function directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the coefficients of logistic regression\n",
    "\n",
    "Logistic regression is most often used to solve classification problems because that allows us to predict the probability of a categorical value Y for a given X. However, instead of simply predicting an outcome, we sometimes want to understand the quantitative impact that a unit change in X has on Y. This is known as inference. \n",
    "\n",
    "To infer meaning from a model, we need to understand what a beta coefficients mean. The way we interpret the beta coefficients depends on whether a predictor X is continuous like age or categorical like gender. \n",
    "\n",
    "Let's start with this scenario where the predictor is continuous. The equation for such a model is shown here. Because the predictor age is a continuous variable, we interpret the beta zero coefficient of -5.95 as a log odds of a customer of age zero renting an e-bike. To understand the intuition behind this interpretation, simply substitute the value zero for age in the equation. This gives us -5.95. Since we are not particularly interested in the log odds of babies renting e-bikes, we'll ignore the beta zero coefficient for the remainder of this analysis. \n",
    "\n",
    "The beta one coefficient is the change in log odds as a result of a unit change in X. For this model, a beta coef-, one coefficient of 0.12 tells us that the log odds of renting an e-bike increases by 0.12 for each unit change in age. As you can tell, this is not a natural way to communicate this type of information. \n",
    "\n",
    "Let's interpret the beta one coefficient in terms of odds instead. To do so, we'll exponentiate. Now we can interpret the exponentiated beta one coefficient of 1.13 to mean that for every unit change in age, the odds of renting an e-bike increases by 13%. In other words, if we knew the odds of a 30 year old renting an e-bike, then the beta one coefficient tells us that the odds of a 31 year old renting an e-bike would be 13% higher than that of the 30 year old. Note that we arrived at 13% or 0.13 by subtracting one from 1.13. We did this because exponents beta one is greater than one. If exponent beta one were less than one, we would've subtracted it from one instead. \n",
    "\n",
    "For example, if exponent beta one was 0.84, we would interpret the coefficient to mean that for every unit change in age the odds of renting an e-bike decreases by 16%. We may be tempted to also interpret the beta coefficient in terms of probability by dividing exponent beta one by one plus exponent beta one, which gives us 0.53. However, this value is not the constant change in probability as a result of a unit change in X nor is it the percentage at which the probability changes as a result of a unique change in X. Because the relationship between X and the probability of Y is non-linear the amount of the probability changes due to a unique change in X is dependent on the value of X. \n",
    "\n",
    "To understand the relationship between the beta coefficient and probability, we need to look at the value of the beta one coefficient. What we can infer from the coefficient is that regardless of the value of X, if beta one is positive, then increase in X will result in an increase in the probability of Y. Conversely, if beta one is negative, then increase in X will result in a decrease in the probability of Y. \n",
    "\n",
    "Now, let's go one step further and look at how we interpret the beta one coefficient for a categorical predictor.\n",
    "\n",
    "This time, let's assume we have a logistic regression model like this one, that models the probability of a customer renting an e-bike as a function of their gender. Generally, when we use categorical predictors in regression analysis, the algorithm dummy codes the values of the predictor into binary form as zero and one. For example, instead of a categorical variable called gender with values, male and female, we end up with a dichotomous variable with values zero and one. Where zero means male and one means female. Because the predictor gender is a categorical variable and because the value of the variable is zero for males, we interpret the beta zero coefficient of -0.17 as the log odds of a male customer renting an e-bike. \n",
    "\n",
    "To understand the intuition behind this interpretation, simply substitute the value zero for gender in the equation and we end up with -0.17. The beta one coefficient of 0.6 is the change in log odds between males and females in terms of renting an e-bike. \n",
    "\n",
    "As we came to realize previously, interpreting coefficient in terms of log odds is rather clumsy. Let's look at the odds instead. If we exponentiate beta zero, we get 0.84. This means that according to our model, the odds of a male customer renting an e-bike is 0.84. Because this value is less than one, it means that men are less likely than women to rent an e-bike. If we exponentiate beta one, we get 1.82. This means that according to our model, the odds of a female customer renting an e-bike in comparison to a male customer is 1.82 to one. In other words, women are 82% more likely to rent an e-bike than men. Similar to the interpretation for a continuous predictor, if beta one is positive, then we can infer that being female, which is denoted by X equal to one, increases the probability of renting an e-bike. Conversely, if beta one were negative, then we would infer that being female would decrease the probability of renting an e-bike. The inference we make based on probability will always be consistent with that of the odds ratio. As a result, we often do not attempt to interpret the coefficients in terms of probability. Odds are enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why and when to use logistic regression\n",
    "\n",
    "In order to know when to and when not to use logistic regression, we need to understand its strengths and weaknesses. \n",
    "\n",
    "STRENGS\n",
    "\n",
    "In terms of strengths, similar to other regression methods, logistic regression models are easy to train. They do not require hyper parameter tuning. Logistic regression models are efficient in that they are not computationally expensive. Unlike some other machine learning algorithms, the predictive variables we use to train a logistic regression model do not have to be scaled before being used. The predictive value and the coefficients of a logistic regression model are easy to understand and interpret. Logistic regression models work best with continuous predictive variables, however they can handle a reasonable number of categorical predictors as well. \n",
    "\n",
    "WEAKNESSES\n",
    "\n",
    " There are some weaknesses inherent in logistic regression models. Logistic regression models tend to underperform when there are multiple or non-linear decision boundaries within the data. As a parametric model, logistic regression requires that we make assumptions about the form of the data before beginning the modeling process. For example, we have to decide which type of regression to use based on the assumed relationship between the predictors and the response variable. As a result of the assumptions we make in logistic regression, our model may not be able to capture some of the complex or subtle patterns in the data. Logistic regression is sensitive to outliers and cannot deal with missing data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
